{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: J.Lee, KAIST (Korea), 2020.\n",
    "\n",
    "Y.Yang, Multi-Dimensional Atomic Imaging Lab, KAIST\n",
    "\n",
    "DL augmentation code\n"   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.io\n",
    "import time\n",
    "from torch.utils import data\n",
    "\n",
    "import DL_aug as DLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking that cuda is available or not ###\n",
    "USE_CUDA=torch.cuda.is_available()\n",
    "DEVICE=torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"CUDA: {}\".format(USE_CUDA))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "### input parameter for training ###\n",
    "N_of_data=10000;  # training set. number\n",
    "N_of_vdata=1000;  # validation set, number\n",
    "N_of_tdata=1000;  # test set, number\n",
    "\n",
    "batch_size=1;  # batch_size\n",
    "N_epoch=100;     # epoch\n",
    "data_size=144;  # data_size (ex 144x144x144 volume -> 144)\n",
    "###\n",
    "\n",
    "### input & output folder path ###\n",
    "INPUT_PATH='input data path';\n",
    "TARGET_PATH='target data path';\n",
    "###\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data loading setting ###\n",
    "params1 = {'INPUT_FILE_NAME':'Pt_input', 'TARGET_FILE_NAME' : 'Pt_target','INPUT_INSIDE_NAME':'GF_Vol', 'TARGET_INSIDE_NAME':'target'}\n",
    "params2 = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 12}\n",
    "\n",
    "N_of_start=1\n",
    "train_ID_set = range(N_of_start,N_of_start+N_of_data);\n",
    "validation_ID_set = range(N_of_start+N_of_data,N_of_start+N_of_data+N_of_vdata)\n",
    "test_ID_set = range(N_of_start+N_of_data+N_of_vdata,N_of_start+N_of_data+N_of_vdata+N_of_tdata)\n",
    "\n",
    "\n",
    "train_dataset = DLa.Dataset(train_ID_set,INPUT_PATH, TARGET_PATH, DLa.image_shift(4), **params1)\n",
    "validation_dataset = DLa.Dataset(validation_ID_set,INPUT_PATH, TARGET_PATH, DLa.image_shift(4), **params1)\n",
    "test_dataset = DLa.Dataset(test_ID_set,INPUT_PATH, TARGET_PATH, None, **params1)\n",
    "\n",
    "\n",
    "train_generator = data.DataLoader(train_dataset, **params2)\n",
    "validation_generator = data.DataLoader(validation_dataset, **params2)\n",
    "test_generator = data.DataLoader(test_dataset, **params2)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate model \n",
    "aut = DLa.UnetGenerator_3d(in_dim=1,out_dim=1,num_filter=12).to(DEVICE)\n",
    "print(\"model contructing: OK!\")\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define loss function & optimizer ###\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(aut.parameters(), lr=0.0002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer = optim.SGD(aut.parameters(), lr=0.05, momentum=0.9)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training process ###\n",
    "print(\"Training starts\")\n",
    "\n",
    "epoch_print=5000//batch_size;\n",
    "total_startTime = time.time()\n",
    "for epoch in range(N_epoch):  # loop over the dataset multiple times\n",
    "    aut.train()\n",
    "    startTime = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, target) in enumerate(train_generator):\n",
    "        # input & target data\n",
    "        inputs=inputs.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "        target=target.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = aut(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print train imformation\n",
    "        running_loss += (loss.item())**0.5\n",
    "        #print(running_loss)\n",
    "        #if i % epoch_print== epoch_print-1:    # print every 1000 mini-batches\n",
    "        ##print('[i: %d, %4d %%] loss: %.10f' %(i + 1, (i + 1)/N_of_data*batch_size*100, running_loss / epoch_print))\n",
    "        #running_loss = 0.0\n",
    "\n",
    "    endTime = time.time() - startTime\n",
    "\n",
    "\n",
    "    # calculating loss of training set & validation set \n",
    "    aut.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum_test=0\n",
    "        for j, (inputs, target) in enumerate(train_generator):\n",
    "            inputs=inputs.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "            target=target.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "\n",
    "            outputs = aut(inputs)\n",
    "            loss_test = criterion(outputs, target)\n",
    "            loss_sum_test += (loss_test.item())**0.5 # MSE -> RMSE\n",
    "        print('[epoch: %d, %3d %%] training set loss: %.10f '  %(epoch + 1, (epoch + 1)/N_epoch*100 , loss_sum_test/(j+1)))\n",
    "\n",
    "        loss_sum_test=0\n",
    "        for j, (inputs, target) in enumerate(validation_generator):\n",
    "            inputs=inputs.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "            target=target.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "\n",
    "            outputs = aut(inputs)\n",
    "            loss_test = criterion(outputs, target)\n",
    "            loss_sum_test += (loss_test.item())**0.5 # MSE -> RMSE\n",
    "        print('[epoch: %d, %3d %%] validation set loss: %.10f time: %.3f '  %(epoch + 1, (epoch + 1)/N_epoch*100 , loss_sum_test/(j+1) ,endTime))\n",
    "\n",
    "total_endTime = time.time() - total_startTime\n",
    "print('Training has been finished')\n",
    "print('Total time: %.3f'  %(total_endTime))\n",
    "###\n",
    "\n",
    "\n",
    "### calculate loss of test set ####\n",
    "aut.eval()\n",
    "with torch.no_grad():\n",
    "    loss_sum_test=0\n",
    "    for j, (inputs, target) in enumerate(test_generator):\n",
    "        inputs=inputs.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "        target=target.view(-1,1,data_size,data_size,data_size).float().to(DEVICE);\n",
    "\n",
    "        outputs = aut(inputs)\n",
    "        loss_test = criterion(outputs, target)\n",
    "        loss_sum_test += (loss_test.item())**0.5 # MSE -> RMSE\n",
    "    print('test set loss: %.10f '  %(loss_sum_test/(j+1)))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the result ###\n",
    "PATH = './DL_aug_save_file'\n",
    "#torch.save(aut.state_dict(), PATH)\n",
    "#print('saving model: OK!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_pytorch_1.1",
   "language": "python",
   "name": "pytorch_1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
